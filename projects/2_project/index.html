<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>NeuPI is a PyTorch-based library for solving inference tasks in Probabilistic Models (PMs) using neural network solvers. It provides a modular, self-supervised framework where the probabilistic model itself supplies the supervisory signal, eliminating the need for labeled training data.</p> <p>NeuPI trains neural solvers to answer queries such as Most Probable Explanation (MPE), Constrained MPE, and Marginal MAP by directly maximizing the log-likelihood of the solutions they propose.</p> <h3 id="links">Links</h3> <ul> <li> <strong>Documentation</strong>: <a href="https://neupi.readthedocs.io/en/latest/" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">https://neupi.readthedocs.io/en/latest/</code></a> </li> <li> <strong>GitHub repository</strong>: <a href="https://github.com/Shivvrat/NeuPI" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">https://github.com/Shivvrat/NeuPI</code></a> </li> <li> <strong>Zenodo record / DOI</strong>: <a href="https://zenodo.org/doi/10.5281/zenodo.15873631" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">https://zenodo.org/doi/10.5281/zenodo.15873631</code></a> </li> </ul> <h3 id="key-features">Key Features</h3> <ul> <li> <strong>Self-supervised training</strong>: Train neural solvers using only the probabilistic model; no labeled data is required.</li> <li> <strong>Advanced inference</strong>: Includes the ITSELF (Inference Time Self-Supervised Training) engine for test-time refinement and improved accuracy.</li> <li> <strong>Discretization</strong>: Offers <code class="language-plaintext highlighter-rouge">ThresholdDiscretizer</code>, <code class="language-plaintext highlighter-rouge">KNearestDiscretizer</code>, and <code class="language-plaintext highlighter-rouge">OAUAI</code> for turning continuous outputs into high-scoring discrete assignments.</li> <li> <strong>Extensible design</strong>: Register custom components (evaluators, embedders, solvers, discretizers, trainers) via a factory-based modular API.</li> </ul> <h3 id="quick-start-conceptual-workflow">Quick Start (Conceptual Workflow)</h3> <ol> <li> <p><strong>Load a probabilistic model (PM)</strong><br> Use an evaluator such as <code class="language-plaintext highlighter-rouge">MarkovNetwork</code> or <code class="language-plaintext highlighter-rouge">SumProductNetwork</code> to score complete assignments via log-likelihood. This evaluator defines the objective; the training loss is the negative log-likelihood of the assignment proposed by the neural solver.</p> </li> <li> <p><strong>Define a neural solver and embedder</strong><br> Specify a neural architecture (e.g., an MLP) that takes an inference query (evidence variables, query variables, evidence values) and outputs a candidate solution. Use an embedder such as <code class="language-plaintext highlighter-rouge">DiscreteEmbedder</code> to map the raw query into feature tensors suitable for the network.</p> </li> <li> <p><strong>Train with <code class="language-plaintext highlighter-rouge">SelfSupervisedTrainer</code></strong><br> The trainer queries the solver for a solution, scores it with the PM evaluator, and backpropagates the negative log-likelihood to update the network.</p> </li> <li> <strong>Perform inference</strong> <ul> <li> <code class="language-plaintext highlighter-rouge">SinglePassInferenceEngine</code>: one forward pass for fast MPE / MMAP inference.</li> <li> <code class="language-plaintext highlighter-rouge">ITSELF_Engine</code>: test-time self-supervised fine-tuning for each query, improving solution quality by iteratively optimizing against the PM.</li> </ul> </li> <li> <strong>Discretize the output</strong><br> Convert continuous probabilities into binary assignments using: <ul> <li> <code class="language-plaintext highlighter-rouge">ThresholdDiscretizer</code> for speed,</li> <li> <code class="language-plaintext highlighter-rouge">KNearestDiscretizer</code> or <code class="language-plaintext highlighter-rouge">OAUAI</code> for higher-quality discrete solutions.</li> </ul> </li> </ol> <h3 id="examples-in-the-examples-directory">Examples (in the <code class="language-plaintext highlighter-rouge">examples/</code> directory)</h3> <ul> <li> <strong>Example 1</strong>: Compute the negative log-likelihood (loss) of a solution to an MPE query on Markov Networks (PGMs) and Sum-Product Networks (probabilistic circuits).</li> <li> <strong>Example 2</strong>: Train a neural solver for MPE queries on Markov Networks and Sum-Product Networks.</li> <li> <strong>Example 3</strong>: Run inference using a trained neural solver on Markov Networks and Sum-Product Networks.</li> <li> <strong>Example 4</strong>: Discretize probabilities into binary assignments using <code class="language-plaintext highlighter-rouge">ThresholdDiscretizer</code>, <code class="language-plaintext highlighter-rouge">KNearestDiscretizer</code>, and <code class="language-plaintext highlighter-rouge">OAUAI</code>.</li> </ul> <h3 id="implemented-methods-and-references">Implemented Methods and References</h3> <table> <thead> <tr> <th>Type</th> <th>Component</th> <th>Primary reference(s)</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>Trainer</td> <td><code class="language-plaintext highlighter-rouge">SelfSupervisedTrainer</code></td> <td>Arya et al., AAAI 2024; Arya et al., NeurIPS 2024; Arya et al., AISTATS 2025 (SINE)</td> <td>Core self-supervised training loop using negative log-likelihood from a PM evaluator (<code class="language-plaintext highlighter-rouge">MarkovNetwork</code>, <code class="language-plaintext highlighter-rouge">SumProductNetwork</code>).</td> </tr> <tr> <td>Loss</td> <td> <code class="language-plaintext highlighter-rouge">MarkovNetwork</code>, <code class="language-plaintext highlighter-rouge">SumProductNetwork</code> </td> <td>Arya et al., AAAI 2024; Arya et al., NeurIPS 2024; Arya et al., AISTATS 2025 (SINE)</td> <td>Compute negative log-likelihood for a given probabilistic model; used as the loss signal.</td> </tr> <tr> <td>Embedding</td> <td><code class="language-plaintext highlighter-rouge">DiscreteEmbedder</code></td> <td>Arya et al., AAAI 2024; Arya et al., NeurIPS 2024</td> <td>Creates discrete input representations from variable assignments and bucket information (evidence, query, unobserved).</td> </tr> <tr> <td>Inference</td> <td><code class="language-plaintext highlighter-rouge">SinglePassInferenceEngine</code></td> <td>Arya et al., AAAI 2024; Arya et al., AISTATS 2025 (SINE)</td> <td>Single-pass inference pipeline for MPE / MMAP.</td> </tr> <tr> <td>Inference</td> <td><code class="language-plaintext highlighter-rouge">ITSELF_Engine</code></td> <td>Arya et al., NeurIPS 2024</td> <td>Inference Time Self-Supervised Learning fine-tuning engine for per-instance optimization.</td> </tr> <tr> <td>Discretizer</td> <td><code class="language-plaintext highlighter-rouge">ThresholdDiscretizer</code></td> <td>Arya et al., AAAI 2024; Arya et al., NeurIPS 2024</td> <td>Simple threshold-based discretization.</td> </tr> <tr> <td>Discretizer</td> <td><code class="language-plaintext highlighter-rouge">KNearestDiscretizer</code></td> <td>Arya et al., AISTATS 2025 (SINE)</td> <td>Beam search over k-nearest binary vectors to find high-quality discrete assignments.</td> </tr> <tr> <td>Discretizer</td> <td><code class="language-plaintext highlighter-rouge">OAUAI</code></td> <td>Arya et al., AISTATS 2025 (SINE)</td> <td>Oracle-based heuristic that focuses queries on variables with highest uncertainty.</td> </tr> </tbody> </table> <h3 id="license">License</h3> <p>NeuPI is released under the MIT License (see the <code class="language-plaintext highlighter-rouge">LICENSE</code> file in the GitHub repository).</p> <h3 id="disclaimer">Disclaimer</h3> <p>This codebase was developed for research purposes and may not strictly adhere to all production coding guidelines; use at your own risk.</p> <h3 id="acknowledgments">Acknowledgments</h3> <p>This work was supported in part by the DARPA Perceptually-Enabled Task Guidance (PTG) Program (HR00112220005), the DARPA Assured Neuro Symbolic Learning and Reasoning (ANSR) Program (HR001122S0039), the National Science Foundation grant IIS-1652835, and the AFOSR award FA9550-23-1-0239.</p> </body></html>